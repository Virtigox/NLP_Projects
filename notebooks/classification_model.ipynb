{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary packages\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "file_path = \"../data/Fake_Real_News/fake_or_real_news.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# This will be result of classification: REAL/FAKE\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Create trainning and test sets: \n",
    "x_train, x_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size= 0.33, random_state= 53)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
      " '000km']\n",
      "  (1, 42470)\t0.07711040274149526\n",
      "  (1, 12105)\t0.15008066461476866\n",
      "  (1, 54177)\t0.13782629144711137\n",
      "  (1, 50628)\t0.061296988343109586\n",
      "  (1, 15924)\t0.3479045460649079\n",
      "  (1, 44520)\t0.4973826512693341\n",
      "  (1, 51896)\t0.11596517664605868\n",
      "  (1, 35783)\t0.30902690818827977\n",
      "  (1, 35256)\t0.12628385718450857\n",
      "  (1, 21881)\t0.21271688045815978\n",
      "  (1, 42534)\t0.06081715886809217\n",
      "  (1, 8399)\t0.08729542880625335\n",
      "  (1, 29531)\t0.1454406205718245\n",
      "  (1, 15927)\t0.4973826512693341\n",
      "  (1, 25686)\t0.13550453594288983\n",
      "  (1, 49203)\t0.1672740861784377\n",
      "  (1, 16814)\t0.10404977746548139\n",
      "  (1, 36087)\t0.12648679854389897\n",
      "  (1, 21568)\t0.1007920919566398\n",
      "  (1, 25684)\t0.1030420922189754\n",
      "  (1, 38823)\t0.06048803110658644\n",
      "  (1, 47506)\t0.14539060877460044\n",
      "  (1, 36831)\t0.10772488937433067\n",
      "  (2, 16972)\t0.1606296088662543\n",
      "  (2, 762)\t0.48803966069171073\n",
      "  :\t:\n",
      "  (4, 19325)\t0.05452053080897492\n",
      "  (4, 7259)\t0.06755319386644243\n",
      "  (4, 51456)\t0.06475353785981061\n",
      "  (4, 7426)\t0.06511222583761835\n",
      "  (4, 39829)\t0.05465988305311523\n",
      "  (4, 9486)\t0.0498567667112753\n",
      "  (4, 27568)\t0.028302541956335657\n",
      "  (4, 48702)\t0.1743065129793454\n",
      "  (4, 41541)\t0.03347420115927609\n",
      "  (4, 20268)\t0.040257433434577744\n",
      "  (4, 44567)\t0.04521244574721562\n",
      "  (4, 7184)\t0.04119900385804159\n",
      "  (4, 15056)\t0.06406962747278261\n",
      "  (4, 23980)\t0.040601095306683384\n",
      "  (4, 36845)\t0.03921825297423197\n",
      "  (4, 37249)\t0.04750657931325537\n",
      "  (4, 12692)\t0.0406789533096042\n",
      "  (4, 43655)\t0.07010870747834863\n",
      "  (4, 4093)\t0.059095267180806606\n",
      "  (4, 26966)\t0.031877860469596585\n",
      "  (4, 9120)\t0.02872653761009892\n",
      "  (4, 5676)\t0.059095267180806606\n",
      "  (4, 22019)\t0.07963078672826533\n",
      "  (4, 43462)\t0.12234830953369169\n",
      "  (4, 23468)\t0.03469208821573642\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = \"english\", max_df = 0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(x_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "#print(tfidf_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
      " '000km']\n",
      "  (1, 42470)\t1\n",
      "  (1, 12105)\t1\n",
      "  (1, 54177)\t1\n",
      "  (1, 50628)\t1\n",
      "  (1, 15924)\t2\n",
      "  (1, 44520)\t2\n",
      "  (1, 51896)\t2\n",
      "  (1, 35783)\t4\n",
      "  (1, 35256)\t1\n",
      "  (1, 21881)\t1\n",
      "  (1, 42534)\t1\n",
      "  (1, 8399)\t1\n",
      "  (1, 29531)\t2\n",
      "  (1, 15927)\t2\n",
      "  (1, 25686)\t1\n",
      "  (1, 49203)\t2\n",
      "  (1, 16814)\t1\n",
      "  (1, 36087)\t1\n",
      "  (1, 21568)\t1\n",
      "  (1, 25684)\t1\n",
      "  (1, 38823)\t1\n",
      "  (1, 47506)\t1\n",
      "  (1, 36831)\t1\n",
      "  (2, 16972)\t1\n",
      "  (2, 762)\t1\n",
      "  :\t:\n",
      "  (4243, 41435)\t1\n",
      "  (4243, 53607)\t1\n",
      "  (4243, 659)\t1\n",
      "  (4243, 38834)\t1\n",
      "  (4243, 19003)\t1\n",
      "  (4243, 11415)\t1\n",
      "  (4243, 7545)\t1\n",
      "  (4243, 22426)\t1\n",
      "  (4243, 54007)\t1\n",
      "  (4243, 7113)\t1\n",
      "  (4243, 4932)\t1\n",
      "  (4243, 39497)\t1\n",
      "  (4243, 50053)\t1\n",
      "  (4243, 38849)\t1\n",
      "  (4243, 20702)\t1\n",
      "  (4243, 42139)\t1\n",
      "  (4243, 17247)\t1\n",
      "  (4243, 50052)\t1\n",
      "  (4243, 55228)\t1\n",
      "  (4243, 29255)\t1\n",
      "  (4243, 49435)\t1\n",
      "  (4243, 11257)\t1\n",
      "  (4243, 52945)\t1\n",
      "  (4243, 20905)\t1\n",
      "  (4243, 7962)\t1\n"
     ]
    }
   ],
   "source": [
    "# Initialzie a CounterVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Transform the training data uising only the 'text' column values: count_train\n",
    "count_train = count_vectorizer.fit_transform(x_train.values)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test\n",
    "count_test = count_vectorizer.transform(x_test.values)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "print(count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "\n",
      "   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
      "0    0     0   0   0   0        0   0    0        0      0  \n",
      "1    0     0   0   0   0        0   0    0        0      0  \n",
      "2    0     0   0   0   0        0   0    0        0      0  \n",
      "3    0     0   0   0   0        0   0    0        0      0  \n",
      "4    0     0   0   0   0        0   0    0        0      0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "\n",
      "   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
      "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns= count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns= tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(tfidf_df.columns) - set(count_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893352462936394\n",
      "[[ 865  143]\n",
      " [  80 1003]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels = ['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8565279770444764\n",
      "[[ 739  269]\n",
      " [  31 1052]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.6150167384026781\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.8976566236250598\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8938307030129125\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8900047824007652\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8857006217120995\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8842659014825442\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.874701099952176\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8703969392635102\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8660927785748446\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8589191774270684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.9/lib/python3.12/site-packages/sklearn/naive_bayes.py:898: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha = alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.280753302177917, '00000031'), (-11.280753302177917, '00006'), (-11.280753302177917, '000ft'), (-11.280753302177917, '001'), (-11.280753302177917, '002'), (-11.280753302177917, '003'), (-11.280753302177917, '006'), (-11.280753302177917, '008'), (-11.280753302177917, '010'), (-11.280753302177917, '013'), (-11.280753302177917, '025'), (-11.280753302177917, '027'), (-11.280753302177917, '035'), (-11.280753302177917, '037'), (-11.280753302177917, '040'), (-11.280753302177917, '044'), (-11.280753302177917, '048'), (-11.280753302177917, '066'), (-11.280753302177917, '068'), (-11.280753302177917, '075')]\n",
      "REAL [(-8.036772745824805, 'president'), (-8.022187159522364, 'american'), (-8.013319806154511, 'media'), (-8.007761560290644, 'donald'), (-8.006632122322646, 'october'), (-7.98962322303076, 'government'), (-7.929695447721539, 'like'), (-7.922750601304927, 'war'), (-7.915731838943572, 'new'), (-7.908889774759155, 'world'), (-7.885018054191407, 'just'), (-7.758145325115569, 'said'), (-7.7498037548099585, 'russia'), (-7.697669509488481, 'fbi'), (-7.604825769578616, '2016'), (-7.554879292243166, 'election'), (-7.541640806988918, 'people'), (-7.235945549755579, 'hillary'), (-6.923220068888362, 'clinton'), (-6.867377223688766, 'trump')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted((zip(nb_classifier.feature_log_prob_[0],feature_names)))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
