{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Hardcoded the installation and dowloaded file\n",
    "\n",
    "# Define where to store the NLTK data (within workspace for write permissions)\n",
    "nltk_data_dir = os.path.expanduser(\"~/workspace/nltk_data\")\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Download 'punkt' and 'punkt_tab' in the defined directory\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import os \n",
    "\n",
    "# Create an array of token without any stopwords and lowercase\n",
    "def createTokensWithoutStopwords(file_path):\n",
    "    # Read the text from the text file\n",
    "    document_path = file_path\n",
    "    document = \"\"\n",
    "    with open(document_path, mode='r', encoding='utf-8') as file:\n",
    "        document = file.read()\n",
    "\n",
    "    # Tokenizing text from the document\n",
    "    tokenized_docs = [word for word in word_tokenize(document.lower()) if word.isalpha()]\n",
    "    no_stop = [token for token in tokenized_docs if token not in stopwords.words('english')]\n",
    "\n",
    "    return no_stop\n",
    "\n",
    "\n",
    "# Counter object which is a dictionary with word and its count\n",
    "def creatBagOfWord(tokenized_doc):\n",
    "\n",
    "    # Instantiate the WordNetLemmatizer\n",
    "    # This will find the base form of the word- 'cats': 'cat', 'running':'run'\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize all the tokens into a new list: lemmatized\n",
    "    lemmatized = [ wordnet_lemmatizer.lemmatize(t) for t in tokenized_doc]\n",
    "\n",
    "    # Create the bag-of-words: bow\n",
    "    bow = Counter(lemmatized)\n",
    "\n",
    "    return bow\n",
    "\n",
    "\n",
    "# Creating different text files into multi-dimensional array of tokens\n",
    "def createTokensOfDifferentArticles(dir_path):\n",
    "    tokenOfArticles = []\n",
    "    filename = []\n",
    "    files = os.listdir(dir_path)\n",
    "    for file in files:\n",
    "        file_path = dir_path + \"/\" + file\n",
    "        tokens = createTokensWithoutStopwords(file_path)\n",
    "        tokenOfArticles.append(tokens)\n",
    "        filename.append(file)\n",
    "    return (tokenOfArticles,filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "# Create a corpus, a bag of word with integer IDs.\n",
    "\n",
    "# Giving words integer IDs\n",
    "def createDictionary(article_tokens):\n",
    "    articles = article_tokens\n",
    "    # create a dictionary out of the article\n",
    "    dictionary = Dictionary(articles)\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "# Create a coprus - a dictionary with integer IDs, and frequency counts\n",
    "def createCorpus(dictionary_bow, article_tokens):\n",
    "    corpus = [dictionary_bow.doc2bow(article) for article in article_tokens]\n",
    "    return corpus\n",
    "\n",
    "# Getting information of words from corpus\n",
    "# the words need to be provided in lowercase\n",
    "def getWordInfoFromCorpus(corpus_data, dictionary_data, word, filenames):\n",
    "    word_id = dictionary_data.token2id.get(word)\n",
    "    if word_id is None:\n",
    "        print(f\"'{word}' doesn't exist in the dictionary.\")\n",
    "        return\n",
    "    \n",
    "    found = False\n",
    "    articleCount = 0\n",
    "    fileCount = len(filenames)\n",
    "    for corpus in corpus_data :\n",
    "        # Trun tuple list into dict and look up the value of the word_id\n",
    "        count = dict(corpus).get(word_id)\n",
    "        if count:\n",
    "            print(f\"'{word}' appears {count} times in this '{filenames[articleCount]}'.\")\n",
    "            found = True\n",
    "        articleCount+=1\n",
    "        \n",
    "    if not found:\n",
    "        print(f\"'{word}' doesn't appear in any document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Term Frequency- inverse document freqeuncy \n",
    "# Determine the most important words in each document\n",
    "# The shared word across the documents should be down-weighted \n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# return type will tfidf object, has to call with corpus data\n",
    "def create_Tfidf_data(corpus_data):\n",
    "    tfidf_data = TfidfModel(corpus_data)\n",
    "    return tfidf_data\n",
    "\n",
    "# Extract the top words of the articles\n",
    "def extractTopic(tfidf_corpus, dictionary_data, corpus_data):\n",
    "    topicArticle = []\n",
    "    for corpus in corpus_data:\n",
    "        topic = []\n",
    "        # sort the tfdif data, the calculated value in desecnding order\n",
    "        top_words = sorted(tfidf_corpus[corpus], key= lambda x: x[1], reverse=True)\n",
    "        # extract an appropirate amount of words- 3 words\n",
    "        topWords = top_words[:5]\n",
    "\n",
    "        for word_id, value in topWords:\n",
    "            topic.append(dictionary_data[word_id])\n",
    "        topicArticle.append(topic)\n",
    "\n",
    "    return topicArticle\n",
    "\n",
    "# Describe the topic of the particular articles\n",
    "def topicIdentification(extracted_topic, articleNames):\n",
    "\n",
    "    for count, article in enumerate(articleNames):\n",
    "        # Turn the extract topics into single string\n",
    "        topics = \" \".join(extracted_topic[count])\n",
    "        print(f\"'{topics}' are the topic of {article}.\")\n",
    "        count+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Exploration how TFIDF's math works\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# Building own Tfidf calculation \n",
    "# 1. Compute Term Frequency(TF): TF(t,d) = count of term t in document d / total term in document d\n",
    "# 2. Compute Document Frequency(DF): DF(t) = number of documents containing term t\n",
    "# 3. Compute Inverse Document Frequency(IDF): IDF(t) = log(total number of doucments/ 1+ DF(t)). 1+ make sure no zero division\n",
    "\n",
    "def computeTfidf(corpus_data):\n",
    "\n",
    "    # number of documents\n",
    "    num_docs = len(corpus_data)\n",
    "\n",
    "    # Counting the occurrence of the word across the document.\n",
    "    df = defaultdict(int)\n",
    "    for doc in corpus_data:\n",
    "        for word_id, freq in doc:\n",
    "            df[word_id] += 1    \n",
    "\n",
    "    \n",
    "    # TF-IDF for each doucment\n",
    "    tfidf_corpus = []\n",
    "    for doc in corpus_data:\n",
    "        # tfidf values for each document\n",
    "        doc_tfidf = []\n",
    "        # consider the second value of the doc: doc is a tuple\n",
    "        total_terms = sum(count for _, count in doc)\n",
    "\n",
    "        for word_id, count in doc:\n",
    "            # Term Frequency\n",
    "            tf = count/ total_terms\n",
    "\n",
    "            # Inverse Document Frequency\n",
    "            idf = math.log(num_docs/(1+df[word_id]))\n",
    "\n",
    "            # TF-IDF\n",
    "            tfidf = tf * idf\n",
    "            doc_tfidf.append((word_id,tfidf))\n",
    "\n",
    "        tfidf_corpus.append(doc_tfidf)\n",
    "\n",
    "    return  tfidf_corpus\n",
    "        \n",
    "computeTfidf(corpus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'college quincy massachusetts community university' are the topic of QuincyCollege_data.txt.\n",
      "'applause thank america nation ever' are the topic of WH_inauguration_data.txt.\n",
      "'q leavitt karoline trump room' are the topic of WH_briefingStatement.txt.\n"
     ]
    }
   ],
   "source": [
    "# tokenize the different articles\n",
    "# Return tuple with two array: n-dimensional array of tokens and  array of filename\n",
    "token_articles, filename = createTokensOfDifferentArticles(\"../data/Custom_Articles\")\n",
    "\n",
    "# Getting corpus data\n",
    "dictionary_data = createDictionary(token_articles)\n",
    "corpus_data = createCorpus(dictionary_data, token_articles)\n",
    "\n",
    "\n",
    "#getWordInfoFromCorpus(corpus_data, dictionary_data, 'deepseek', filename)\n",
    "\n",
    "# Calculate the value of tfidf for each words in each articles\n",
    "tfidf_data = create_Tfidf_data(corpus_data)\n",
    "\n",
    "# extract the topic\n",
    "topicOfArticle = extractTopic(tfidf_data, dictionary_data, corpus_data)\n",
    "\n",
    "# identify the topics of each articles\n",
    "topicIdentification(topicOfArticle, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
